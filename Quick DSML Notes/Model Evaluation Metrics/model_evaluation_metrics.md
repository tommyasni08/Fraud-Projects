# ğŸ“Š Model Evaluation Metrics

Machine learning metrics help us understand how good a modelâ€™s predictions are. They differ based on model task type.

# ğŸ§  Types of Model Evaluation Problems

| Problem Type                | What We Measure                        | Typical Metrics                                           |
| --------------------------- | -------------------------------------- | --------------------------------------------------------- |
| Classification              | Correctly identify classes             | Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC, LogLoss |
| Regression                  | Predict continuous numeric values      | MAE, MSE, RMSE, RÂ², MAPE                                  |
| Ranking / Recommendation    | Correctly rank relevant items          | Precision@k, Recall@k, MAP, NDCG                          |
| Clustering                  | Meaningful separation into groups      | Silhouette Score, Daviesâ€“Bouldin, Calinskiâ€“Harabasz       |
| Probabilistic / Forecasting | Good uncertainty/probability estimates | Brier Score, Pinball Loss, CRPS                           |

# âœ… Classification Metrics

ğŸ“Œ Confusion Matrix (Foundation for all metrics)
| | Predicted Positive | Predicted Negative |
| - | - | - |
| Actual Positive |TP |FN|
| Actual Negative |FP |TN|

## ğŸŸ¢ Accuracy

**Formula**: (TP + TN) / (TP + FP + TN + FN)

**Meaning**: Fraction of all correct predictions.

âœ… Good when classes are balanced.

âš ï¸ Bad when classes are imbalanced (e.g., fraud detection).

**Memory trick**: â€œHow accurate overall?â€

## ğŸŸ£ Precision

**Formula**: TP / (TP + FP)

**Meaning**: Of all predicted positives, how many were actually positive.

âœ… Good when false alarms are costly (e.g., banning legitimate users).

**Memory trick**: Precision â†’ â€œHow precise were your positive guesses?â€

## ğŸ”µ Recall (Sensitivity / True Positive Rate)

**Formula**: TP / (TP + FN)

**Meaning**: Of all actual positives, how many did you catch.

âœ… Good when missing a positive is costly (e.g., missing fraud).

**Memory trick**: Recall â†’ â€œHow much did you recall from all real positives?â€

## ğŸ”¶ F1-Score

**Formula**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

**Meaning**: Harmonic mean of precision and recall.

âœ… Good balance when both FP and FN matter.

**Memory trick**: F1 â†’ â€œFair 1-number trade-off.â€

## ğŸŸ  ROC-AUC (Receiver Operating Characteristic â€“ Area Under Curve)

**Meaning**: Measures ranking ability â€” probability that a random positive is ranked higher than a random negative.

âœ… Good overall discrimination metric.

**Memory trick**: ROC = â€œRanking Of Classes.â€

## ğŸŸ¡ PR-AUC (Precisionâ€“Recall AUC)

**Meaning**: Focuses on the trade-off between precision and recall.

âœ… Better than ROC-AUC for imbalanced data.

**Memory trick**: â€œPR-AUC zooms in on positive class performance.â€

## ğŸ”´ Log-Loss / Cross-Entropy

**Meaning**: Punishes confident wrong predictions heavily.

âœ… Used when model outputs probabilities.

**Memory trick**: â€œWrong and confident = logarithmically painful.â€

# ğŸ“ˆ Regression Metrics

| Metric                          | Interpretation                | Memory Trick             |
| ------------------------------- | ----------------------------- | ------------------------ |
| MAE Mean Absolute Error         | Average absolute error        | â€œSimple average gapâ€     |
| MSE Mean Squared Error          | Penalizes large errors        | â€œBig mistakes hurt moreâ€ |
| RMSE Root MSE                   | Same unit as target           | â€œReal-world error sizeâ€  |
| RÂ² Coefficient of Determination | % variance explained by model | â€œCloser to 1 = betterâ€   |
| MAPE Percentage Error           | % error, intuitive            | â€œPercentage missâ€        |

# ğŸ§© Ranking / Recommendation Metrics

| Metric                                       | Meaning                                                |
| -------------------------------------------- | ------------------------------------------------------ |
| Precision@k                                  | Of the top-k items recommended, how many are relevant. |
| Recall@k                                     | Of all relevant items, how many appear in top-k.       |
| MAP (Mean Average Precision)                 | Average precision over ranked results.                 |
| NDCG (Normalized Discounted Cumulative Gain) | Rewards correct ordering â€” higher rank = higher gain.  |

Memory trick:
â€œP@k cares about top-k quality, NDCG cares about top-k ordering.â€

# ğŸŒ€ Clustering / Unsupervised Metrics

| Metric               | Meaning / Intuition                                                                            | Good When                      | Better Value |
| -------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------ | ------------ |
| Silhouette Score     | Measures how similar an item is to its own cluster vs. others (âˆ’1 to 1). â€œ1 = well-clustered.â€ | Distinct clusturs              | Higher       |
| Daviesâ€“Bouldin Index | Lower = better separation between clusters.                                                    | Smaller spread within clusters | Lower        |
| Calinskiâ€“Harabasz    | Higher = better defined clusters.                                                              | Better Separation              | Higher       |

# ğŸŒ¤ Probabilistic & Forecasting Metrics

| Metric       | Meaning                                                                                    |
| ------------ | ------------------------------------------------------------------------------------------ |
| Brier Score  | Mean squared difference between predicted probabilities and actual outcomes (0 = perfect). |
| Pinball Loss | Used in quantile regression (forecasts).                                                   |
| CRPS         | Extends Brier to continuous distributions.                                                 |

# âœ… Quick Direction Guide

| Metric Type | Higher is Better                                     | Lower is Better                |
| ----------- | ---------------------------------------------------- | ------------------------------ |
| Success     | Accuracy, Precision, Recall, F1, AUC, RÂ², Silhouette | â€”                              |
| Error       | â€”                                                    | MSE, RMSE, MAE, Logloss, Brier |

# ğŸ’¡ How to Choose a Metric Quickly

| Scenario                          | Best Metric                   |
| --------------------------------- | ----------------------------- |
| Balanced Classification           | Accuracy                      |
| Imbalanced Classification (fraud) | Recall, Precision, F1, PR-AUC |
| Numeric Prediction                | RMSE + RÂ²                     |
| Top-k Recommendations             | Precision@k / NDCG            |
| Clustering                        | Silhouette Score              |

# Bonus Summary table for Functions in metrics.py

| Function            | Task          | What it Evaluates                   |
| ------------------- | ------------- | ----------------------------------- |
| precision_at_k      | Ranking       | Quality of top-k predictions        |
| recall_at_k         | Ranking       | Coverage of relevant items          |
| average_precision   | Ranking       | Ranking quality of relevant items   |
| map_at_k            | Ranking       | Mean performance across users       |
| dcg_at_k            | Ranking       | Ordering discount (higher = better) |
| ndcg_at_k           | Ranking       | Normalized ideal rank (0â€“1)         |
| evaluate_clustering | Clustering    | Separation & density metrics        |
| brier_score         | Probabilistic | Probability accuracy                |
| mean_pinball_loss_q | Forecasting   | Quantile prediction accuracy        |
| as_pretty_dict      | Utility       | Pretty formatting                   |
| \_safe_round        | Utility       | Safe numeric rounding               |
